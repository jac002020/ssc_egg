{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahsavari/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import slp_data\n",
    "import config\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split recordings to train test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recs = []\n",
    "for f in os.listdir(config.data_dir):\n",
    "    if f.endswith('.lbl'):\n",
    "        all_recs.append(f.split('.')[0])\n",
    "all_recs = np.asarray(all_recs, dtype=np.str)\n",
    "num_recs = len(all_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_steps = 1000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "strides = 1\n",
    "#pooling kernel size and sride\n",
    "k = 2\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 30*250 #\n",
    "num_classes = len(slp_data.DataSet.all_labels)  #\n",
    "dropout = 0.2  # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32,[None, num_input], name='image')\n",
    "Y = tf.placeholder(tf.int32,[None, num_classes], name='label')\n",
    "keep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)\n",
    "\n",
    "#your codes here\n",
    "#  Store layers weight & bias\n",
    "# The first two convolutional layer\n",
    "w_c_1 = tf.Variable(tf.random_normal([100, 1, 1, 32], stddev=0.01))\n",
    "w_c_2 = tf.Variable(tf.random_normal([100, 1, 32, 64], stddev=0.01))\n",
    "b_c_1 = tf.Variable(tf.random_normal([32], stddev=0.1))\n",
    "b_c_2 = tf.Variable(tf.random_normal([64], stddev=0.1))\n",
    "\n",
    "# The second two convolutional layer weights\n",
    "w_c_3 = tf.Variable(tf.random_normal([100, 1, 64, 64], stddev=0.01))\n",
    "w_c_4 = tf.Variable(tf.random_normal([100, 1, 64, 64], stddev=0.01))\n",
    "b_c_3 = tf.Variable(tf.random_normal([64], stddev=0.1))\n",
    "b_c_4 = tf.Variable(tf.random_normal([64], stddev=0.1))\n",
    "\n",
    "# Fully connected weight\n",
    "w_f_1 = tf.Variable(tf.random_normal([125*15*64, 1024], stddev=0.01))\n",
    "b_f_1 = tf.Variable(tf.random_normal([1024], stddev=0.1))\n",
    "\n",
    "# output layer weight\n",
    "w_out = tf.Variable(tf.random_normal([1024, num_classes], stddev=0.01))\n",
    "b_out = tf.Variable(tf.random_normal([num_classes], stddev=0.01))\n",
    "\n",
    "#\n",
    "# Define model\n",
    "x =tf.reshape(X, shape=[-1,250*30,1,1])\n",
    "# first layer convolution\n",
    "conv1 = tf.nn.conv2d(x, w_c_1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv1 = tf.add(conv1, b_c_1)\n",
    "# second layer convolution\n",
    "conv2 = tf.nn.conv2d(conv1, w_c_2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv2 = tf.add(conv2, b_c_2)\n",
    "# first Max Pooling (down-sampling)\n",
    "pool_1 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# third layer convolution\n",
    "conv3 = tf.nn.conv2d(pool_1, w_c_3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv3 = tf.add(conv3, b_c_3)\n",
    "# fourth layer convolution\n",
    "conv4 = tf.nn.conv2d(conv3, w_c_4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv4 = tf.add(conv4, b_c_4)\n",
    "\n",
    "# second Max Pooling (down-sampling)\n",
    "pool_2 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# first Fully connected layer\n",
    "# Reshape conv4 output to fit fully connected layer input and first fully connected layer\n",
    "fc1 = tf.reshape(pool_2, [-1, w_f_1.get_shape().as_list()[0]])\n",
    "fc1 = tf.nn.relu(tf.add(tf.matmul(fc1, w_f_1), b_f_1))\n",
    "# Apply Dropout\n",
    "fc1 = tf.nn.dropout(fc1,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-9e6e0f3979e9>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#your codes here\n",
    "# Output, class prediction\n",
    "logits = tf.add(tf.matmul(fc1, w_out), b_out)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.argmax(prediction, axis=1)\n",
    "y_true = tf.argmax(Y, axis=1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_pred, y_true), tf.float32))\n",
    "# accuracy, accuracy_op = tf.metrics.accuracy(labels=y_true, predictions=correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train recordings: ['slp16' 'slp01b' 'slp02a' 'slp03' 'slp14' 'slp41' 'slp59' 'slp48' 'slp32'\n",
      " 'slp67x' 'slp37' 'slp61' 'slp45' 'slp66' 'slp60']\n",
      "valid recordings: ['slp04']\n",
      "test recordings: ['slp02b' 'slp01a']\n",
      "recording   1/15 : ./datasets/MIT-BIH-true/slp32 with x,y shapes: (640, 7500) (640,)\n",
      "recording   2/15 : ./datasets/MIT-BIH-true/slp67x with x,y shapes: (154, 7500) (154,)\n",
      "recording   3/15 : ./datasets/MIT-BIH-true/slp60 with x,y shapes: (710, 7500) (710,)\n",
      "recording   4/15 : ./datasets/MIT-BIH-true/slp41 with x,y shapes: (780, 7500) (780,)\n",
      "recording   5/15 : ./datasets/MIT-BIH-true/slp61 with x,y shapes: (720, 7500) (720,)\n",
      "recording   6/15 : ./datasets/MIT-BIH-true/slp37 with x,y shapes: (698, 7500) (698,)\n",
      "recording   7/15 : ./datasets/MIT-BIH-true/slp01b with x,y shapes: (360, 7500) (360,)\n",
      "recording   8/15 : ./datasets/MIT-BIH-true/slp02a with x,y shapes: (360, 7500) (360,)\n",
      "recording   9/15 : ./datasets/MIT-BIH-true/slp45 with x,y shapes: (760, 7500) (760,)\n",
      "recording  10/15 : ./datasets/MIT-BIH-true/slp48 with x,y shapes: (760, 7500) (760,)\n",
      "recording  11/15 : ./datasets/MIT-BIH-true/slp03 with x,y shapes: (720, 7500) (720,)\n",
      "recording  12/15 : ./datasets/MIT-BIH-true/slp59 with x,y shapes: (458, 7500) (458,)\n",
      "recording  13/15 : ./datasets/MIT-BIH-true/slp14 with x,y shapes: (714, 7500) (714,)\n",
      "recording  14/15 : ./datasets/MIT-BIH-true/slp66 with x,y shapes: (439, 7500) (439,)\n",
      "recording  15/15 : ./datasets/MIT-BIH-true/slp16 with x,y shapes: (694, 7500) (694,)\n",
      "x,y has been read with shape: (8967, 7500) (8967, 8)\n",
      "recording   1/1 : ./datasets/MIT-BIH-true/slp04 with x,y shapes: (720, 7500) (720,)\n",
      "x,y has been read with shape: (720, 7500) (720, 8)\n",
      "Batch     0/5 , Minibatch Loss= 443857.8750 Test Accuracy= 0.1719\n",
      "Batch     1/5 , Minibatch Loss= 379097.1875 Test Accuracy= 0.2188\n",
      "Batch     2/5 , Minibatch Loss= 343956.3125 Test Accuracy= 0.2812\n",
      "Batch     3/5 , Minibatch Loss= 425086.8125 Test Accuracy= 0.2109\n",
      "Batch     4/5 , Minibatch Loss= 354940.6875 Test Accuracy= 0.2500\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-930ce4c4a7db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             print(\"Step {:5d}/{} , Minibatch Loss={:12.4f} Training Accuracy={:7.4f} Valid Loss={:12.4f} Valid Accuracy={:7.4f}\"\n\u001b[0;32m---> 93\u001b[0;31m                   .format(step, total_steps, loss_train, acc_train, loss_valid, acc_valid))\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0myellow_cards\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpatient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_valid' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def print_measures(predicted_labels, true_labels):\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    print(cm)\n",
    "    print(classification_report(true_labels, predicted_labels))\n",
    "    print(\"Accuracy : {}\".format(accuracy_score(true_labels, predicted_labels)))\n",
    "\n",
    "def run_prediction(test_data, sess):\n",
    "    n_examples = test_data.get_examples_count()\n",
    "    n_batches = n_examples // batch_size\n",
    "    predicted_labels = np.empty(shape=(0), dtype=np.int)\n",
    "    y_test_all = np.empty(shape=(0), dtype=np.int)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for i in range(n_batches):\n",
    "        x_test, y_test = test_data.next_batch(batch_size, shuffle=False)\n",
    "        loss,acc,pr,cpr,y_t=sess.run([loss_op,accuracy,prediction,correct_pred,y_true],feed_dict={X:x_test,Y:y_test,keep_prob:1.0})\n",
    "        predicted_labels = np.concatenate((predicted_labels, cpr))\n",
    "        y_test_all = np.concatenate((y_test_all, y_t))\n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "        # print(\"Batch {:5d}/{} , Minibatch Loss={:12.4f} Test Accuracy={:7.4f}\".format(i, n_batches, loss, acc))\n",
    "    total_loss /= n_batches\n",
    "    total_acc /= n_batches\n",
    "    \n",
    "    return predicted_labels, y_test_all, total_loss, total_acc\n",
    "    \n",
    "def compute_measures_on_set(test_data, sess):\n",
    "    predicted_labels, y_test_all, loss, acc = run_prediction(test_data, sess)\n",
    "    print(\"average loss: {} average accuracy: {}\".format(loss, acc))    \n",
    "    print_measures(predicted_labels, y_test_all[:len(predicted_labels)])\n",
    "    return predicted_labels, y_test_all, loss, acc\n",
    "        \n",
    "\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "for train_records_index, test_records_index in kf.split(all_recs):\n",
    "    \n",
    "    train_recs = all_recs[train_records_index]\n",
    "    test_recs = all_recs[test_records_index]\n",
    "    \n",
    "    valid_recs = train_recs[-1:]\n",
    "    train_recs = train_recs[:-1]\n",
    "    print(\"train recordings: {}\".format(train_recs))\n",
    "    print(\"valid recordings: {}\".format(valid_recs))\n",
    "    print(\"test recordings: {}\".format(test_recs))\n",
    "    \n",
    "    data = slp_data.SlpDataSet(config.data_dir, \n",
    "                               train_recs,\n",
    "                               valid_recs, \n",
    "                               test_recs, one_hot=True)\n",
    "    \n",
    "    \n",
    "    sess=tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    step = 1\n",
    "    epochs = 20\n",
    "    \n",
    "    n_train_examples = data.train.get_examples_count()\n",
    "    total_steps = epochs * (n_train_examples // batch_size)\n",
    "    \n",
    "    patient = 10\n",
    "    yellow_cards = 0\n",
    "    prev_loss_val = np.inf\n",
    "    \n",
    "    learning_rate_value = 0.002\n",
    "    learning_rate_decay = 0.9\n",
    "    \n",
    "    for step in range(total_steps):\n",
    "        batch_x, batch_y = data.train.next_batch(batch_size, shuffle=True)\n",
    "        \n",
    "        _ = sess.run(train_op, \n",
    "                     feed_dict = {X: batch_x, Y: batch_y, keep_prob: 1.0, learning_rate: learning_rate_value})\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            \n",
    "            trainpl, traintl, loss_train, acc_train = sess.run([correct_pred, y_true, loss_op, accuracy],\n",
    "                                               feed_dict = {X:batch_x,Y:batch_y,keep_prob:1.0})\n",
    "\n",
    "            vpl, vtl, loss_val, acc_val = run_prediction(data.validation, sess)\n",
    "            \n",
    "            if loss_val > prev_loss_val:\n",
    "                learning_rate_value *= learning_rate_decay\n",
    "                if loss_train < 2.0:\n",
    "                    yellow_cards += 1\n",
    "                print(\"Valid loss not decreasing. yellow cards={} new learning rate={}\"\n",
    "                     .format(yellow_cards, learning_rate_value))\n",
    "            prev_loss_val = loss_val    \n",
    "            print(\"Step {:5d}/{} , Minibatch Loss={:12.4f} Training Accuracy={:7.4f} Valid Loss={:12.4f} Valid Accuracy={:7.4f}\"\n",
    "                  .format(step, total_steps, loss_train, acc_train, loss_val, acc_val))\n",
    "            \n",
    "            if yellow_cards == patient:\n",
    "                print(\"Early Stoppping...\")\n",
    "                break\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    tpl, ttl, loss_test, acc_test = compute_measures_on_set(data.test, sess)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
