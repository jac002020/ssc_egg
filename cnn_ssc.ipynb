{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahsavari/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import slp_data\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split recordings to train test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17]), 'TEST:', array([0, 1]))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-17fbf8a176e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_records_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_records_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_recs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAIN:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_records_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TEST:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_records_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_recs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_recs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_records_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_recs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_recs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_records_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "all_recs = []\n",
    "for f in os.listdir(config.data_dir):\n",
    "    if f.endswith('.lbl'):\n",
    "        all_recs.append(f.split('.')[0])\n",
    "all_recs = np.asarray(all_recs, dtype=np.str)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "for train_records_index, test_records_index in kf.split(all_recs):\n",
    "    print(\"TRAIN:\", train_records_index, \"TEST:\", test_records_index)\n",
    "    train_recs = all_recs[train_records_index]\n",
    "    test_recs = all_recs[test_records_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_recs = len(all_recs)\n",
    "\n",
    "train_recs = all_recs[:3*num_recs//5]\n",
    "val_recs = all_recs[3*num_recs//5:4*num_recs//5]\n",
    "test_recs = all_recs[4*num_recs//5:]\n",
    "\n",
    "print(\"train recordings: {}\".format(train_recs))\n",
    "print(\"valid recordings: {}\".format(val_recs))\n",
    "print(\"test recordings: {}\".format(test_recs))\n",
    "\n",
    "data = slp_data.SlpDataSet(config.data_dir, \n",
    "                           train_recs,\n",
    "                           val_recs, \n",
    "                           test_recs, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_steps = 1000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "strides = 1\n",
    "#pooling kernel size and sride\n",
    "k = 2\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 30*250 #\n",
    "num_classes = len(slp_data.DataSet.all_labels)  #\n",
    "dropout = 0.2  # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='image')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='label')\n",
    "keep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)\n",
    "\n",
    "#your codes here\n",
    "#  Store layers weight & bias\n",
    "# The first two convolutional layer\n",
    "w_c_1 = tf.Variable(tf.random_normal([50, 1, 1, 32], stddev=0.01))\n",
    "w_c_2 = tf.Variable(tf.random_normal([50, 1, 32, 64], stddev=0.01))\n",
    "b_c_1 = tf.Variable(tf.random_normal([32], stddev=0.1))\n",
    "b_c_2 = tf.Variable(tf.random_normal([64], stddev=0.1))\n",
    "\n",
    "# The second two convolutional layer weights\n",
    "w_c_3 = tf.Variable(tf.random_normal([50, 1, 64, 64], stddev=0.01))\n",
    "w_c_4 = tf.Variable(tf.random_normal([50, 1, 64, 64], stddev=0.01))\n",
    "b_c_3 = tf.Variable(tf.random_normal([64], stddev=0.1))\n",
    "b_c_4 = tf.Variable(tf.random_normal([64], stddev=0.1))\n",
    "\n",
    "# Fully connected weight\n",
    "w_f_1 = tf.Variable(tf.random_normal([125*15*64, 1024], stddev=0.01))\n",
    "b_f_1 = tf.Variable(tf.random_normal([1024], stddev=0.1))\n",
    "\n",
    "# output layer weight\n",
    "w_out = tf.Variable(tf.random_normal([1024, num_classes], stddev=0.01))\n",
    "b_out = tf.Variable(tf.random_normal([num_classes], stddev=0.01))\n",
    "\n",
    "#\n",
    "# Define model\n",
    "x =tf.reshape(X,shape=[-1,250*30,1,1])\n",
    "# first layer convolution\n",
    "conv1 = tf.nn.conv2d(x, w_c_1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv1 = tf.add(conv1, b_c_1)\n",
    "# second layer convolution\n",
    "conv2 = tf.nn.conv2d(conv1, w_c_2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv2 = tf.add(conv2, b_c_2)\n",
    "# first Max Pooling (down-sampling)\n",
    "pool_1 = tf.nn.avg_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# third layer convolution\n",
    "conv3 = tf.nn.conv2d(pool_1, w_c_3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv3 = tf.add(conv3, b_c_3)\n",
    "# fourth layer convolution\n",
    "conv4 = tf.nn.conv2d(conv3, w_c_4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv4 = tf.add(conv4, b_c_4)\n",
    "\n",
    "# second Max Pooling (down-sampling)\n",
    "pool_2 = tf.nn.avg_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# first Fully connected layer\n",
    "# Reshape conv4 output to fit fully connected layer input and first fully connected layer\n",
    "fc1 = tf.reshape(pool_2, [-1, w_f_1.get_shape().as_list()[0]])\n",
    "fc1 = tf.nn.relu(tf.add(tf.matmul(fc1, w_f_1), b_f_1))\n",
    "# Apply Dropout\n",
    "fc1 = tf.nn.dropout(fc1,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-7488f175abe1>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#your codes here\n",
    "# Output, class prediction\n",
    "logits = tf.add(tf.matmul(fc1, w_out), b_out)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.argmax(prediction, axis=1)\n",
    "y_true = tf.argmax(Y, axis=1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(correct_pred, y_true), tf.float32))\n",
    "# accuracy, accuracy_op = tf.metrics.accuracy(labels=y_true, predictions=correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording   1/10 : ./MIT-BIH-true/slp59 with x,y shapes: (458, 7500) (458,)\n",
      "recording   2/10 : ./MIT-BIH-true/slp02a with x,y shapes: (360, 7500) (360,)\n",
      "recording   3/10 : ./MIT-BIH-true/slp14 with x,y shapes: (714, 7500) (714,)\n",
      "recording   4/10 : ./MIT-BIH-true/slp32 with x,y shapes: (640, 7500) (640,)\n",
      "recording   5/10 : ./MIT-BIH-true/slp16 with x,y shapes: (694, 7500) (694,)\n",
      "recording   6/10 : ./MIT-BIH-true/slp48 with x,y shapes: (760, 7500) (760,)\n",
      "recording   7/10 : ./MIT-BIH-true/slp01b with x,y shapes: (360, 7500) (360,)\n",
      "recording   8/10 : ./MIT-BIH-true/slp67x with x,y shapes: (154, 7500) (154,)\n",
      "recording   9/10 : ./MIT-BIH-true/slp41 with x,y shapes: (780, 7500) (780,)\n",
      "recording  10/10 : ./MIT-BIH-true/slp03 with x,y shapes: (720, 7500) (720,)\n",
      "x,y has been read with shape: (5640, 7500) (5640, 7)\n",
      "Step     0/880 , Minibatch Loss=     37.7677 Training Accuracy= 0.2812\n",
      "Step     1/880 , Minibatch Loss=    259.1886 Training Accuracy= 0.3359\n",
      "Step    10/880 , Minibatch Loss=   1575.1829 Training Accuracy= 0.2969\n",
      "Step    20/880 , Minibatch Loss=    568.7354 Training Accuracy= 0.1094\n",
      "Step    30/880 , Minibatch Loss=    188.0625 Training Accuracy= 0.1250\n",
      "Step    40/880 , Minibatch Loss=     61.5657 Training Accuracy= 0.2578\n",
      "Step    50/880 , Minibatch Loss=     61.8361 Training Accuracy= 0.3984\n",
      "Step    60/880 , Minibatch Loss=     20.7175 Training Accuracy= 0.3359\n",
      "Step    70/880 , Minibatch Loss=      7.3680 Training Accuracy= 0.4141\n",
      "Step    80/880 , Minibatch Loss=      5.7417 Training Accuracy= 0.2734\n",
      "Step    90/880 , Minibatch Loss=      2.7753 Training Accuracy= 0.2188\n",
      "Step   100/880 , Minibatch Loss=      1.9347 Training Accuracy= 0.3516\n",
      "Step   110/880 , Minibatch Loss=      1.4191 Training Accuracy= 0.4531\n",
      "Step   120/880 , Minibatch Loss=      1.5231 Training Accuracy= 0.3359\n",
      "Step   130/880 , Minibatch Loss=      1.3226 Training Accuracy= 0.4453\n",
      "Step   140/880 , Minibatch Loss=      1.3090 Training Accuracy= 0.4375\n",
      "Step   150/880 , Minibatch Loss=      1.0831 Training Accuracy= 0.5391\n",
      "Step   160/880 , Minibatch Loss=      1.2172 Training Accuracy= 0.5312\n",
      "Step   170/880 , Minibatch Loss=      1.2084 Training Accuracy= 0.5000\n",
      "Step   180/880 , Minibatch Loss=      1.0477 Training Accuracy= 0.5938\n",
      "Step   190/880 , Minibatch Loss=      1.1108 Training Accuracy= 0.5312\n",
      "Step   200/880 , Minibatch Loss=      1.0472 Training Accuracy= 0.6328\n",
      "Step   210/880 , Minibatch Loss=      1.0010 Training Accuracy= 0.6406\n",
      "Step   220/880 , Minibatch Loss=      0.8176 Training Accuracy= 0.6719\n",
      "Step   230/880 , Minibatch Loss=      1.0862 Training Accuracy= 0.6094\n",
      "Step   240/880 , Minibatch Loss=      1.0876 Training Accuracy= 0.6172\n",
      "Step   250/880 , Minibatch Loss=      0.9673 Training Accuracy= 0.6016\n",
      "Step   260/880 , Minibatch Loss=      0.9554 Training Accuracy= 0.6719\n",
      "Step   270/880 , Minibatch Loss=      0.9181 Training Accuracy= 0.6406\n",
      "Step   280/880 , Minibatch Loss=      0.8146 Training Accuracy= 0.7109\n",
      "Step   290/880 , Minibatch Loss=      0.7987 Training Accuracy= 0.7500\n",
      "Step   300/880 , Minibatch Loss=      0.8368 Training Accuracy= 0.6875\n",
      "Step   310/880 , Minibatch Loss=      0.8073 Training Accuracy= 0.7031\n",
      "Step   320/880 , Minibatch Loss=      0.8189 Training Accuracy= 0.6953\n",
      "Step   330/880 , Minibatch Loss=      0.8227 Training Accuracy= 0.7031\n",
      "Step   340/880 , Minibatch Loss=      0.8134 Training Accuracy= 0.7656\n",
      "Step   350/880 , Minibatch Loss=      0.7413 Training Accuracy= 0.7109\n",
      "Step   360/880 , Minibatch Loss=      0.7393 Training Accuracy= 0.7578\n",
      "Step   370/880 , Minibatch Loss=      0.6277 Training Accuracy= 0.7734\n",
      "Step   380/880 , Minibatch Loss=      0.7552 Training Accuracy= 0.7500\n",
      "Step   390/880 , Minibatch Loss=      0.7688 Training Accuracy= 0.7188\n",
      "Step   400/880 , Minibatch Loss=      0.6932 Training Accuracy= 0.7969\n",
      "Step   410/880 , Minibatch Loss=      0.6997 Training Accuracy= 0.7266\n",
      "Step   420/880 , Minibatch Loss=      0.7242 Training Accuracy= 0.7422\n",
      "Step   430/880 , Minibatch Loss=      0.6410 Training Accuracy= 0.8047\n",
      "Step   440/880 , Minibatch Loss=      0.5167 Training Accuracy= 0.8750\n",
      "Step   450/880 , Minibatch Loss=      0.7071 Training Accuracy= 0.7656\n",
      "Step   460/880 , Minibatch Loss=      0.7442 Training Accuracy= 0.7812\n",
      "Step   470/880 , Minibatch Loss=      0.6065 Training Accuracy= 0.7891\n",
      "Step   480/880 , Minibatch Loss=      0.5946 Training Accuracy= 0.8594\n",
      "Step   490/880 , Minibatch Loss=      0.5908 Training Accuracy= 0.8359\n",
      "Step   500/880 , Minibatch Loss=      0.5390 Training Accuracy= 0.8594\n",
      "Step   510/880 , Minibatch Loss=      0.5705 Training Accuracy= 0.8281\n",
      "Step   520/880 , Minibatch Loss=      0.5473 Training Accuracy= 0.8125\n",
      "Step   530/880 , Minibatch Loss=      0.5382 Training Accuracy= 0.8438\n",
      "Step   540/880 , Minibatch Loss=      0.4911 Training Accuracy= 0.8125\n",
      "Step   550/880 , Minibatch Loss=      0.5715 Training Accuracy= 0.8516\n",
      "Step   560/880 , Minibatch Loss=      0.5296 Training Accuracy= 0.8359\n",
      "Step   570/880 , Minibatch Loss=      0.4760 Training Accuracy= 0.8828\n",
      "Step   580/880 , Minibatch Loss=      0.4584 Training Accuracy= 0.8672\n",
      "Step   590/880 , Minibatch Loss=      0.4042 Training Accuracy= 0.8984\n",
      "Step   600/880 , Minibatch Loss=      0.5070 Training Accuracy= 0.8516\n",
      "Step   610/880 , Minibatch Loss=      0.4793 Training Accuracy= 0.8594\n",
      "Step   620/880 , Minibatch Loss=      0.4431 Training Accuracy= 0.8828\n",
      "Step   630/880 , Minibatch Loss=      0.3943 Training Accuracy= 0.8906\n",
      "Step   640/880 , Minibatch Loss=      0.4704 Training Accuracy= 0.8828\n",
      "Step   650/880 , Minibatch Loss=      0.4300 Training Accuracy= 0.8828\n",
      "Step   660/880 , Minibatch Loss=      0.3213 Training Accuracy= 0.9375\n",
      "Step   670/880 , Minibatch Loss=      0.4568 Training Accuracy= 0.8516\n",
      "Step   680/880 , Minibatch Loss=      0.5116 Training Accuracy= 0.8594\n",
      "Step   690/880 , Minibatch Loss=      0.3910 Training Accuracy= 0.8672\n",
      "Step   700/880 , Minibatch Loss=      0.3671 Training Accuracy= 0.9141\n",
      "Step   710/880 , Minibatch Loss=      0.3547 Training Accuracy= 0.9297\n",
      "Step   720/880 , Minibatch Loss=      0.3496 Training Accuracy= 0.8984\n",
      "Step   730/880 , Minibatch Loss=      0.3741 Training Accuracy= 0.9062\n",
      "Step   740/880 , Minibatch Loss=      0.3602 Training Accuracy= 0.8828\n",
      "Step   750/880 , Minibatch Loss=      0.3421 Training Accuracy= 0.8984\n",
      "Step   760/880 , Minibatch Loss=      0.3167 Training Accuracy= 0.9141\n",
      "Step   770/880 , Minibatch Loss=      0.3794 Training Accuracy= 0.8828\n",
      "Step   780/880 , Minibatch Loss=      0.3454 Training Accuracy= 0.8906\n",
      "Step   790/880 , Minibatch Loss=      0.3024 Training Accuracy= 0.9297\n",
      "Step   800/880 , Minibatch Loss=      0.2872 Training Accuracy= 0.9297\n",
      "Step   810/880 , Minibatch Loss=      0.2474 Training Accuracy= 0.9453\n",
      "Step   820/880 , Minibatch Loss=      0.3090 Training Accuracy= 0.9062\n",
      "Step   830/880 , Minibatch Loss=      0.2965 Training Accuracy= 0.9141\n",
      "Step   840/880 , Minibatch Loss=      0.2753 Training Accuracy= 0.9219\n",
      "Step   850/880 , Minibatch Loss=      0.2214 Training Accuracy= 0.9375\n",
      "Step   860/880 , Minibatch Loss=      0.3041 Training Accuracy= 0.9375\n",
      "Step   870/880 , Minibatch Loss=      0.2782 Training Accuracy= 0.8984\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Start training\n",
    "sess=tf.Session()\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "step = 1\n",
    "epochs = 20\n",
    "n_train_examples = data.train.get_examples_count()\n",
    "total_steps = epochs * ( n_train_examples // batch_size )\n",
    "for step in range(total_steps):\n",
    "    batch_x, batch_y = data.train.next_batch(batch_size, shuffle=True)\n",
    "    # Run optimization op (backprop)\n",
    "    _ = sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss,acc,pr,cpr,y_t=sess.run([loss_op,accuracy,prediction,correct_pred,y_true],feed_dict={X:batch_x,Y:batch_y,keep_prob:1.0})\n",
    "        print(\"Step {:5d}/{} , Minibatch Loss={:12.4f} Training Accuracy={:7.4f}\".format(step, total_steps, loss, acc))\n",
    "\n",
    "    #print(cpr)\n",
    "    #print(y_t)\n",
    "\n",
    "print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording   1/4 : ./MIT-BIH-true/slp01a with x,y shapes: (240, 7500) (240,)\n",
      "recording   2/4 : ./MIT-BIH-true/slp60 with x,y shapes: (710, 7500) (710,)\n",
      "recording   3/4 : ./MIT-BIH-true/slp04 with x,y shapes: (720, 7500) (720,)\n",
      "recording   4/4 : ./MIT-BIH-true/slp66 with x,y shapes: (439, 7500) (439,)\n",
      "x,y has been read with shape: (2109, 7500) (2109, 7)\n",
      "Batch     0/16 , Minibatch Loss=      0.2027 Test Accuracy= 0.9609\n",
      "Batch     1/16 , Minibatch Loss=      0.2614 Test Accuracy= 0.9141\n",
      "Batch     2/16 , Minibatch Loss=      0.2706 Test Accuracy= 0.9219\n",
      "Batch     3/16 , Minibatch Loss=      0.2530 Test Accuracy= 0.9219\n",
      "Batch     4/16 , Minibatch Loss=      0.2509 Test Accuracy= 0.9219\n",
      "Batch     5/16 , Minibatch Loss=      0.2475 Test Accuracy= 0.9531\n",
      "Batch     6/16 , Minibatch Loss=      0.2019 Test Accuracy= 0.9688\n",
      "Batch     7/16 , Minibatch Loss=      0.2782 Test Accuracy= 0.9219\n",
      "Batch     8/16 , Minibatch Loss=      0.2644 Test Accuracy= 0.9375\n",
      "Batch     9/16 , Minibatch Loss=      0.2011 Test Accuracy= 0.9609\n",
      "Batch    10/16 , Minibatch Loss=      0.2675 Test Accuracy= 0.9297\n",
      "Batch    11/16 , Minibatch Loss=      0.2870 Test Accuracy= 0.9297\n",
      "Batch    12/16 , Minibatch Loss=      0.2601 Test Accuracy= 0.9141\n",
      "Batch    13/16 , Minibatch Loss=      0.3144 Test Accuracy= 0.8906\n",
      "Batch    14/16 , Minibatch Loss=      0.2053 Test Accuracy= 0.9375\n",
      "Batch    15/16 , Minibatch Loss=      0.2886 Test Accuracy= 0.9141\n",
      "average loss: 0.253422550857 average accuracy: 0.93115234375\n",
      "[[362  24   0   0   8  14   0]\n",
      " [  1 592   0   0   8   3   0]\n",
      " [  1   7  79   0   0   0   0]\n",
      " [  0   0   0  28   0   0   0]\n",
      " [  2  13   0   0 137   0   0]\n",
      " [  3  49   0   0   8 704   0]\n",
      " [  0   0   0   0   0   0   5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93       408\n",
      "           1       0.86      0.98      0.92       604\n",
      "           2       1.00      0.91      0.95        87\n",
      "           3       1.00      1.00      1.00        28\n",
      "           4       0.85      0.90      0.88       152\n",
      "           5       0.98      0.92      0.95       764\n",
      "           6       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      2048\n",
      "   macro avg       0.95      0.94      0.95      2048\n",
      "weighted avg       0.94      0.93      0.93      2048\n",
      "\n",
      "Accuracy : 0.93115234375\n"
     ]
    }
   ],
   "source": [
    "def print_measures(predicted_labels, true_labels):\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    print(cm)\n",
    "    print(classification_report(true_labels, predicted_labels))\n",
    "    print(\"Accuracy : {}\".format(accuracy_score(true_labels, predicted_labels)))\n",
    "    \n",
    "n_examples = data.test.get_examples_count()\n",
    "import numpy as np\n",
    "n_batches = n_examples // batch_size\n",
    "predicted_labels = np.empty(shape=(0), dtype=np.int)\n",
    "y_test_all = np.empty(shape=(0), dtype=np.int)\n",
    "\n",
    "total_loss = 0.0\n",
    "total_acc = 0.0\n",
    "for i in range(n_batches):\n",
    "    x_test, y_test = data.train.next_batch(batch_size, shuffle=False)\n",
    "    loss,acc,pr,cpr,y_t=sess.run([loss_op,accuracy,prediction,correct_pred,y_true],feed_dict={X:x_test,Y:y_test,keep_prob:1.0})\n",
    "    predicted_labels = np.concatenate((predicted_labels, cpr))\n",
    "    y_test_all = np.concatenate((y_test_all, y_t))\n",
    "    total_loss += loss\n",
    "    total_acc += acc\n",
    "    print(\"Batch {:5d}/{} , Minibatch Loss={:12.4f} Test Accuracy={:7.4f}\".format(i, n_batches, loss, acc))\n",
    "total_loss /= n_batches\n",
    "total_acc /= n_batches\n",
    "print(\"average loss: {} average accuracy: {}\".format(total_loss, total_acc))\n",
    "\n",
    "print_measures(predicted_labels, y_test_all[:len(predicted_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
